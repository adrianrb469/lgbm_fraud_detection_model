{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2ea850",
   "metadata": {},
   "source": [
    "# 03 - Base Model and Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9110e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff74d3",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dec9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/processed/'     \n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, 'transactions_processed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c904df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask  = (df['year']==2020) & (df['trans_month']==12)\n",
    "valid_mask = (df['year']==2020) & (df['trans_month'].between(10,11))\n",
    "train_mask = ~test_mask & ~valid_mask\n",
    "\n",
    "train_df, valid_df, test_df = df[train_mask], df[valid_mask], df[test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bd1c2",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50302274",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'is_fraud'\n",
    "ignore = [target,'transaction_datetime']\n",
    "\n",
    "features = [c for c in df.columns if c not in ignore]\n",
    "cat_cols = [c for c in features if df[c].dtype=='object']\n",
    "\n",
    "#  LightGBM requires categorical features instead of object dtype.\n",
    "for c in cat_cols:\n",
    "    for part in (train_df, valid_df, test_df):\n",
    "        part[c] = part[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b90699",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155553e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights(part, legit_freq_w=10.0):\n",
    "    \"\"\"Boost loss on legitimate frequent‑customer rows.\"\"\"\n",
    "    w = np.ones(len(part))\n",
    "    mask = (part['is_frequent_merchant'] == 1) & (part[target] == 0)\n",
    "    w[mask] = legit_freq_w         \n",
    "    return w\n",
    "\n",
    "w_train = make_weights(train_df, 5.0)\n",
    "w_valid = make_weights(valid_df, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfa665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(train_df[features], label=train_df[target],\n",
    "                        weight=w_train, categorical_feature=cat_cols,\n",
    "                        free_raw_data=False)\n",
    "lgb_valid = lgb.Dataset(valid_df[features], label=valid_df[target],\n",
    "                        weight=w_valid, categorical_feature=cat_cols,\n",
    "                        free_raw_data=False, reference=lgb_train)\n",
    "\n",
    "freq_map = {\n",
    "    id(lgb_train): train_df['is_frequent_merchant'].values,\n",
    "    id(lgb_valid): valid_df['is_frequent_merchant'].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f3d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpi_report(model, df, name, thr):\n",
    "    \"\"\"\n",
    "        Helper function to calculate KPIs\n",
    "        @param model: trained LightGBM model\n",
    "        @param df: dataframe to evaluate\n",
    "        @param name: name of the dataframe\n",
    "        @param thr: threshold for the model\n",
    "\n",
    "        Returns:\n",
    "        - A dictionary containing:\n",
    "            - Recall (fraud detection rate)\n",
    "            - Overall FPR (false positive rate for overall transactions)\n",
    "            - Frequent‑cust FPR (false positive rate for frequent customers)\n",
    "            - Overall FP/TP ratio (overall fraud detection rate)\n",
    "            - Freq‑cust FP/TP ratio (fraud detection rate for frequent customers)\n",
    "    \"\"\"\n",
    "    predictions = model.predict(df[features])\n",
    "    actuals = df[target].values\n",
    "    is_frequent = df[\"is_frequent_merchant\"].values\n",
    "    predicted_fraud = predictions >= thr\n",
    "\n",
    "    tp = ((actuals == 1) & predicted_fraud).sum()\n",
    "    fn = ((actuals == 1) & ~predicted_fraud).sum()\n",
    "    fp = ((actuals == 0) & predicted_fraud).sum()\n",
    "\n",
    "    fp_freq = ((actuals == 0) & predicted_fraud & (is_frequent == 1)).sum()\n",
    "    legitimate_freq = ((actuals == 0) & (is_frequent == 1)).sum()\n",
    "\n",
    "    recall = tp / (tp + fn)\n",
    "    fpr = fp / (actuals == 0).sum()\n",
    "    fpr_freq = fp_freq / legitimate_freq\n",
    "\n",
    "    fp_tp_ratio_overall = (tp + fp) / tp\n",
    "    fp_tp_ratio_freq = (tp + fp_freq) / tp\n",
    "\n",
    "    results = {\n",
    "        \"recall\": recall,\n",
    "        \"fpr\": fpr,\n",
    "        \"fpr_freq\": fpr_freq,\n",
    "        \"fp_tp_ratio_overall\": fp_tp_ratio_overall,\n",
    "        \"fp_tp_ratio_freq\": fp_tp_ratio_freq\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0913a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.963712\n"
     ]
    }
   ],
   "source": [
    "params = dict(objective='binary', metric='auc',\n",
    "              learning_rate=0.05, num_leaves=64,\n",
    "              feature_fraction=0.8, bagging_fraction=0.8,\n",
    "              bagging_freq=5, seed=42, verbosity=-1, scale_pos_weight=50, min_child_weight=0.1)\n",
    "\n",
    "params_base = params.copy()\n",
    "params_base['metric'] = 'auc'  \n",
    "\n",
    "model_base = lgb.train(\n",
    "    params_base,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_valid],\n",
    "    valid_names=['valid'],\n",
    "    num_boost_round=500,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb54e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            recall     fpr  fpr_freq  fp_tp_ratio_overall  fp_tp_ratio_freq\n",
      "Validation  0.8879  0.0076    0.0059               2.7940            1.1462\n",
      "Test        0.8450  0.0084    0.0058               6.3578            1.4908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "validation_results = kpi_report(model_base, valid_df, \"Validation\", 0.9)\n",
    "test_results = kpi_report(model_base, test_df, \"Test\", 0.9)\n",
    "\n",
    "df = pd.DataFrame([validation_results, test_results], index=['Validation', 'Test'])\n",
    "print(df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8933a7",
   "metadata": {},
   "source": [
    "### Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6bca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_tp_ratio_freq(preds, data):\n",
    "    y_true = data.get_label()\n",
    "    freq   = freq_map[id(data)]      # from your earlier `freq_map`\n",
    "    y_pred = preds > 0.50\n",
    "    tp     = ((y_true==1) &  y_pred).sum()\n",
    "    fp_f   = ((y_true==0) &  y_pred & (freq==1)).sum()\n",
    "    ratio  = (tp + fp_f) / tp if tp else np.inf\n",
    "    return 'fp_tp_ratio_freq', ratio, False\n",
    "\n",
    "def fp_tp_ratio(preds, data):\n",
    "    y_true  = data.get_label()\n",
    "    y_pred  = preds > 0.50           # fixed cut‑off inside metric\n",
    "    tp      = ((y_true==1) &  y_pred).sum()\n",
    "    fp      = ((y_true==0) &  y_pred).sum()\n",
    "    ratio   = (tp + fp) / tp if tp else np.inf   # lower = better\n",
    "    return 'fp_tp_ratio', ratio, False\n",
    "\n",
    "def balanced_cost(preds, data, w_fp=3.0, w_fn=10.0):\n",
    "    \"\"\"\n",
    "    Penalizes false positives on frequent customers,\n",
    "    and false negatives (missed frauds).\n",
    "    Higher w_fn puts more pressure on recall.\n",
    "    \"\"\"\n",
    "    y_true = data.get_label()\n",
    "    freq   = freq_map[id(data)]\n",
    "    y_pred = preds > 0.50\n",
    "\n",
    "    fp_freq = ((y_true==0) & y_pred & (freq==1)).sum()\n",
    "    fn      = ((y_true==1) & ~y_pred).sum()\n",
    "    cost    = w_fp * fp_freq + w_fn * fn\n",
    "    return 'balanced_cost', cost, False\n",
    "\n",
    "def f05_score(preds, data):\n",
    "    \"\"\"F‑beta with β=0.5: weigh precision twice recall (good for FP control).\"\"\"\n",
    "    y_true = data.get_label()\n",
    "    y_pred = preds > 0.50\n",
    "    tp = ((y_true==1) & y_pred).sum()\n",
    "    fp = ((y_true==0) & y_pred).sum()\n",
    "    fn = ((y_true==1) & ~y_pred).sum()\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) else 0\n",
    "    beta2 = 0.25         \n",
    "    score = (1 + beta2) * precision * recall / (beta2 * precision + recall) if (precision+recall) else 0\n",
    "    return 'f05_score', -score, True       \n",
    "\n",
    "def freq_fpr(preds, data):\n",
    "    \"\"\" \n",
    "        Frequent customer false positive rate \n",
    "        FP_freq / Legit_freq\n",
    "        \n",
    "    \"\"\"\n",
    "    y_true = data.get_label()\n",
    "    freq   = freq_map[id(data)]\n",
    "    y_pred = preds > 0.50\n",
    "    fp_freq  = ((y_true==0) & y_pred & (freq==1)).sum()\n",
    "    legit_freq = ((y_true==0) & (freq==1)).sum()\n",
    "    fpr = fp_freq / legit_freq if legit_freq else 0\n",
    "    return 'freq_fpr', fpr, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d016cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> fp_tp_ratio\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid's binary_logloss: 0.00759189\tvalid's fp_tp_ratio: 1.51102\n",
      "[200]\tvalid's binary_logloss: 0.00630076\tvalid's fp_tp_ratio: 1.25887\n",
      "[300]\tvalid's binary_logloss: 0.0059466\tvalid's fp_tp_ratio: 1.16632\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid's binary_logloss: 0.00590922\tvalid's fp_tp_ratio: 1.15546\n",
      "Best fp_tp_ratio: 1.1555\n",
      "\n",
      " -> fp_tp_ratio_freq\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid's binary_logloss: 0.00759189\tvalid's fp_tp_ratio_freq: 1.02605\n",
      "[200]\tvalid's binary_logloss: 0.00630076\tvalid's fp_tp_ratio_freq: 1.01461\n",
      "[300]\tvalid's binary_logloss: 0.0059466\tvalid's fp_tp_ratio_freq: 1.01053\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid's binary_logloss: 0.00590922\tvalid's fp_tp_ratio_freq: 1.0084\n",
      "Best fp_tp_ratio_freq: 1.0084\n",
      "\n",
      " -> balanced_cost\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's binary_logloss: 0.0166253\tvalid's balanced_cost: 1088\n",
      "Best balanced_cost: 1088.0000\n",
      "\n",
      " -> f05_score\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's binary_logloss: 0.0435814\tvalid's f05_score: -0.298777\n",
      "Best f05_score: -0.2988\n",
      "\n",
      " -> freq_fpr\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid's binary_logloss: 0.00759189\tvalid's freq_fpr: 0.000876542\n",
      "[200]\tvalid's binary_logloss: 0.00630076\tvalid's freq_fpr: 0.000471984\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid's binary_logloss: 0.00663957\tvalid's freq_fpr: 0.000471984\n",
      "Best freq_fpr: 0.0005\n"
     ]
    }
   ],
   "source": [
    "metrics = [fp_tp_ratio, fp_tp_ratio_freq, balanced_cost, f05_score, freq_fpr]\n",
    "\n",
    "\n",
    "def run_and_log(feval_fn, legit_freq_w=10.0):\n",
    "    w_train = make_weights(train_df, legit_freq_w)\n",
    "    w_valid = make_weights(valid_df, legit_freq_w)\n",
    "    lgb_train.set_weight(w_train)\n",
    "    lgb_valid.set_weight(w_valid)\n",
    "\n",
    "    print(f\"\\n -> {feval_fn.__name__}\")\n",
    "\n",
    "    params = dict(\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        seed=42,\n",
    "        verbosity=-1,\n",
    "        scale_pos_weight=50,\n",
    "        min_child_weight=0.1,\n",
    "    )\n",
    "\n",
    "    mdl = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_valid],\n",
    "        valid_names=[\"valid\"],\n",
    "        feval=feval_fn,\n",
    "        num_boost_round=500,\n",
    "        callbacks=[early_stopping(50), log_evaluation(100)],\n",
    "    )\n",
    "\n",
    "    best = mdl.best_score[\"valid\"][feval_fn.__name__]\n",
    "    print(f\"Best {feval_fn.__name__}: {best:.4f}\")\n",
    "    return mdl, best\n",
    "\n",
    "\n",
    "results = {}\n",
    "models = {}\n",
    "for fe in metrics:\n",
    "    model, score = run_and_log(fe, legit_freq_w=10)  \n",
    "    results[fe.__name__] = score\n",
    "    models[fe.__name__] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19f0e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KPI Reports:\n",
      "\n",
      "Model: fp_tp_ratio\n",
      "            recall     fpr  fpr_freq  fp_tp_ratio_overall  fp_tp_ratio_freq\n",
      "Validation  0.6062  0.0002    0.0001               1.0560            1.0024\n",
      "Test        0.5349  0.0002    0.0000               1.2174            1.0000\n",
      "\n",
      "Model: fp_tp_ratio_freq\n",
      "            recall     fpr  fpr_freq  fp_tp_ratio_overall  fp_tp_ratio_freq\n",
      "Validation  0.6062  0.0002    0.0001               1.0560            1.0024\n",
      "Test        0.5349  0.0002    0.0000               1.2174            1.0000\n",
      "\n",
      "Model: balanced_cost\n",
      "            recall     fpr  fpr_freq  fp_tp_ratio_overall  fp_tp_ratio_freq\n",
      "Validation  0.7847  0.0022    0.0011               1.5846            1.0320\n",
      "Test        0.7287  0.0028    0.0016               3.0904            1.1543\n",
      "\n",
      "Model: f05_score\n",
      "            recall     fpr  fpr_freq  fp_tp_ratio_overall  fp_tp_ratio_freq\n",
      "Validation  0.8599  0.0096     0.006               3.3208            1.1527\n",
      "Test        0.7752  0.0106     0.005               8.3600            1.4600\n",
      "\n",
      "Model: freq_fpr\n",
      "            recall     fpr  fpr_freq  fp_tp_ratio_overall  fp_tp_ratio_freq\n",
      "Validation  0.5855  0.0004    0.0001               1.1259            1.0050\n",
      "Test        0.4922  0.0005    0.0001               1.5118            1.0157\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    validation_results = kpi_report(model, valid_df, \"Validation\", 0.9)\n",
    "    test_results = kpi_report(model, test_df, \"Test\", 0.9)\n",
    "\n",
    "    df = pd.DataFrame([validation_results, test_results], index=['Validation', 'Test'])\n",
    "    all_results[model_name] = df.round(4)\n",
    "\n",
    "print(\"\\nKPI Reports:\")\n",
    "for model_name, df in all_results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
